{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: bagging\n",
      "bayesian_search: True\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bagging\"\n",
    "random_state = 42\n",
    "bayesian_search = True\n",
    "\n",
    "print(f\"model_name: {model_name}\")\n",
    "print(f\"bayesian_search: {bayesian_search}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region: us-east-1\n",
      "Execution role: arn:aws:iam::600890512379:role/service-role/AmazonSageMaker-ExecutionRole-20210115T110258\n",
      "Output folder: s3://petprojects/gscreen/output\n"
     ]
    }
   ],
   "source": [
    "# Get the Amazon SageMaker Boto 3 Client\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from sagemaker.session import s3_input, Session\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "## Region and SageMaker Client\n",
    "region = boto3.Session().region_name\n",
    "smclient = boto3.Session().client('sagemaker')\n",
    "print(f\"Region: {region}\")\n",
    "\n",
    "## Get the SageMaker IAM Execution Role\n",
    "from sagemaker import get_execution_role\n",
    "\"\"\"\n",
    "Get the execution role for the notebook instance. \n",
    "This is the IAM role that you created for your notebook instance.\n",
    "You pass the role to the tuning job. \n",
    "This is the role that SageMaker would use to leverage AWS resources\n",
    "\"\"\" \n",
    "role = get_execution_role()\n",
    "print(f\"Execution role: {role}\")\n",
    "\n",
    "## Specify a Bucket and Data Output Location\n",
    "# bucket = sess.default_bucket() # Replace with your own bucket name if needed\n",
    "bucket = 'petprojects'\n",
    "prefix = 'gscreen'\n",
    "## AWS Sagemaker: Define path to save models\n",
    "output_dir = f\"s3://{bucket}/{prefix}/output\"\n",
    "print(f\"Output folder: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy: 1.19.4\n",
      "Pandas: 1.0.1\n",
      "Matplotlib: 3.1.3\n",
      "Scikit-learn: 0.22.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import urllib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "\n",
    "try:skl = Pipeline.__module__[:Pipeline.__module__.index(\".\")]\n",
    "except:skl = Pipeline.__module__\n",
    "\n",
    "print(f\"Numpy: {np.__version__}\")\n",
    "print(f\"Pandas: {pd.__version__}\")\n",
    "print(f\"Matplotlib: {sys.modules[plt.__package__].__version__}\")\n",
    "print(f\"Scikit-learn: {sys.modules[skl].__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scikit-optimize package exist.\n",
      "0.8.1\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  from skopt import BayesSearchCV\n",
    "  print(f\"The scikit-optimize package exist.\")\n",
    "except Exception as e:\n",
    "  print(f\"Import scikit-optimize error: {e}\")\n",
    "  print(f\"Try to re-install scikit-optimize package\")\n",
    "  ! pip install scikit-optimize\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "print (f\"{sys.modules[BayesSearchCV.__module__[:BayesSearchCV.__module__.index('.')]].__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_fit: (296721, 279), <class 'scipy.sparse.csr.csr_matrix'>\n",
      "X_train: (296727, 279), <class 'scipy.sparse.csr.csr_matrix'>\n",
      "X_val: (5000, 279), <class 'scipy.sparse.csr.csr_matrix'>\n",
      "\n",
      "y_fit: (296721,), <class 'numpy.ndarray'>\n",
      "y_train: (296727,), <class 'numpy.ndarray'>\n",
      "y_val: (5000,), <class 'numpy.ndarray'>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load preprocessed data from S3\n",
    "dir=\"gscreen/data/\"\n",
    "s3client = boto3.client('s3')\n",
    "\n",
    "s3client.download_file(bucket, dir+'X_fit.joblib.compressed', 'X_fit.joblib.compressed')\n",
    "with open(f\"X_fit.joblib.compressed\", \"rb\") as f:\n",
    "    X_fit = joblib.load(f)\n",
    "\n",
    "s3client.download_file(bucket, dir+'X_train.joblib.compressed', 'X_train.joblib.compressed')\n",
    "with open(f\"X_train.joblib.compressed\", \"rb\") as f:\n",
    "    X_train = joblib.load(f)\n",
    "\n",
    "s3client.download_file(bucket, dir+'X_val.joblib.compressed', 'X_val.joblib.compressed')\n",
    "with open(f\"X_val.joblib.compressed\", 'rb') as f:\n",
    "    X_val = joblib.load(f)\n",
    "\n",
    "print(f\"X_fit: {X_fit.shape}, {type(X_fit)}\\\n",
    "\\nX_train: {X_train.shape}, {type(X_train)}\\\n",
    "\\nX_val: {X_val.shape}, {type(X_val)}\\n\")\n",
    "\n",
    "s3client.download_file(bucket, dir+'y_fit.joblib', 'y_fit.joblib')\n",
    "with open(f\"y_fit.joblib\", \"rb\") as f:\n",
    "    y_fit = joblib.load(f)\n",
    "\n",
    "s3client.download_file(bucket, dir+'y_train.joblib', 'y_train.joblib')\n",
    "with open(f\"y_train.joblib\", \"rb\") as f:\n",
    "    y_train = joblib.load(f)\n",
    "\n",
    "s3client.download_file(bucket, dir+'y_val.joblib', 'y_val.joblib')\n",
    "with open(f\"y_val.joblib\", \"rb\") as f:\n",
    "    y_val = joblib.load(f)\n",
    "\n",
    "print(f\"y_fit: {y_fit.shape}, {type(y_fit)}\\\n",
    "\\ny_train: {y_train.shape}, {type(y_train)}\\\n",
    "\\ny_val: {y_val.shape}, {type(y_val)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Define model parameters for starting tuning\n",
    "model_params = {\n",
    "    \"base_estimator\": tree.ExtraTreeRegressor(\n",
    "        criterion=\"mse\",  # {\"mse\", \"friedman_mse\", \"\"mae\"} default=\"mse\"\n",
    "        splitter=\"random\",  # {\"random\", \"best\"} default=\"random\"\n",
    "        max_depth=None,  # default=None\n",
    "        min_samples_split=2,  # default=2\n",
    "        min_samples_leaf=1,  # default=1\n",
    "        random_state=random_state,\n",
    "    ),\n",
    "    \"n_estimators\": 80,\n",
    "    \"max_samples\": 1.0,\n",
    "    \"max_features\": 1.0,\n",
    "    \"bootstrap\": True,\n",
    "    \"bootstrap_features\": False,\n",
    "    \"oob_score\": False,\n",
    "    \"n_jobs\": None,\n",
    "    \"random_state\": random_state,\n",
    "    \"verbose\": 3,\n",
    "}\n",
    "model = ensemble.BaggingRegressor(**model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% ----------------------- Bayesian Optimization ------------------------------\n",
    "# Their core idea of Bayesian Optimization is simple:\n",
    "# when a region of the space turns out to be good, it should be explored more.\n",
    "# Real: Continuous hyperparameter space.\n",
    "# Integer: Discrete hyperparameter space.\n",
    "# Categorical: Categorical hyperparameter space.\n",
    "bayes_space = {\n",
    "    \"n_estimators\": Integer(50, 100),\n",
    "    \"max_samples\": Real(0.5, 1.0),\n",
    "    # \"max_features\": Real(0.7, 1.0),\n",
    "    \"bootstrap\": Categorical([True, False]),\n",
    "    # \"bootstrap_features\": Categorical([True, False]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(real_rates, predicted_rates):\n",
    "    \"\"\"Project's accuracy value estimator\"\"\"\n",
    "    return np.average(abs(real_rates / predicted_rates - 1.0)) * 100.0\n",
    "\n",
    "def calc_metrics(model, X, y):\n",
    "    \"\"\"Calculates result metrics\"\"\"\n",
    "\n",
    "    from sklearn.metrics import max_error\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "    y_true = y\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    me = max_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    mare = accuracy(y_true, y_pred)\n",
    "\n",
    "    print(f\"\\tMax Error: {me}\")\n",
    "    print(f\"\\tMean Absolute Error: {mae}\")\n",
    "    print(f\"\\tRoot Mean Squared Error: {rmse}\")\n",
    "    print(f\"\\tMean Absolute Ratio Error: {mare}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Define custom scorer to evaluate basic models\n",
    "scorer = make_scorer(\n",
    "    score_func=accuracy,\n",
    "    greater_is_better=True,  # Whether score_func is a score function (default),\n",
    "    # meaning high is good, or a loss function, meaning low is good.\n",
    ")\n",
    "# Define classic cross validation method and params\n",
    "cv = model_selection.RepeatedKFold(\n",
    "    n_splits=10,  #! Dont forget\n",
    "    n_repeats=2,\n",
    "    random_state=random_state,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 20 folds for each of 1 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "#%% ----------------------- Bayesian Optimization ------------------------------\n",
    "bs_bp, bs_be = {}, {}\n",
    "if bayesian_search:\n",
    "    tic = time.time()\n",
    "    # Define bayesian search\n",
    "    bayes_search = BayesSearchCV(\n",
    "        model,\n",
    "        search_spaces=bayes_space,\n",
    "        n_iter=30,  #! default=50 Number of parameter settings that are sampled.\n",
    "        scoring=scorer,\n",
    "        n_jobs=None,  # Number of jobs to run in parallel. -1 = using all processors\n",
    "        cv=cv,  # default 3-fold cross validation.\n",
    "        refit=True,\n",
    "        verbose=3,\n",
    "        return_train_score=True,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "    # Make search\n",
    "    model_bayes_search = bayes_search.fit(\n",
    "        X_fit,\n",
    "        y_fit,\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out best parameters\n",
    "if bayesian_search:\n",
    "    bs_bp = model_bayes_search.best_params_\n",
    "    bs_be = model_bayes_search.best_estimator_\n",
    "    print(f\"Bayesian search:\\nBest params are:\\n {bs_bp}\")\n",
    "    print(f\"{bs_be}\")\n",
    "    print(f\"\\nBest score is:\\n{model_bayes_search.best_score_}\")\n",
    "\n",
    "    min, sec = divmod(time.time() - tic, 60)\n",
    "    print(f\"\\nBayesian search taken: {int(min)}min {int(sec)}sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out results\n",
    "if bayesian_search:\n",
    "    model = model_bayes_search.best_estimator_\n",
    "    print(f\"{model_name.title()} Bayesian search:\")\n",
    "    print(\"TRAIN set:\")\n",
    "    calc_metrics(model, X=X_train, y=y_train)\n",
    "    print(\"VALIDATION set:\")\n",
    "    calc_metrics(model, X=X_val, y=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Fit the model w\\o parameters searching -------------------------------------\n",
    "if not bayesian_search:\n",
    "    # Train the model\n",
    "    tic = time.time()\n",
    "    model.fit(X_fit, y_fit)\n",
    "    # Evaluate time spent\n",
    "    min, sec = divmod(time.time() - tic, 60)\n",
    "    print(f\"Time taken: {int(min)}min {int(sec)}sec\")\n",
    "    print(f\"{model}\\n\")\n",
    "\n",
    "    # Print out results\n",
    "    print(f\"{model_name.title()} model:\")\n",
    "    print(\"TRAIN set:\")\n",
    "    calc_metrics(model, X=X_train, y=y_train)\n",
    "    print(\"VALIDATION set:\")\n",
    "    calc_metrics(model, X=X_val, y=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}