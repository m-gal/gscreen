{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS Sagemaker\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from sagemaker.session import s3_input, Session\n",
    "import pandas as pd\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-east-2\n"
     ]
    }
   ],
   "source": [
    "# AWS Sagemaker: Define bucket's name for project\n",
    "bucket = 'gscreenproject'\n",
    "my_region = boto3.session.Session().region_name\n",
    "print(my_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 error: An error occurred (IllegalLocationConstraintException) when calling the CreateBucket operation: The unspecified location constraint is incompatible for the region specific endpoint this request was sent to.\n"
     ]
    }
   ],
   "source": [
    "# # AWS Sagemaker: Create S3 bucket programmly\n",
    "# s3resource = boto3.resource(\"s3\")\n",
    "# #!Regions outside of us-east-1 require the appropriate LocationConstraint to be specified in order to create the bucket in the desired region:\n",
    "# try:\n",
    "#     if my_region == \"us-east-2\":\n",
    "#         s3resource.create_bucket(Bucket=bucket)\n",
    "#         print(f\"S3 bucket {bucket} created successfully\")\n",
    "# except Exception as e:\n",
    "#     print(f\"S3 error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://gscreenproject/xgb_params/output\n"
     ]
    }
   ],
   "source": [
    "# # AWS Sagemaker: Define path to save models\n",
    "# prefix = \"xgb_params\"\n",
    "# output_dir = f\"s3://{bucket}/{prefix}/output\"\n",
    "# print(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # AWS Sagemaker: Downloding a datasets and storing it in S3\n",
    "# try:\n",
    "#     urllib.request.urlretrieve(\"..path to file with data\")\n",
    "#     print(f\"Success: data downloaded to S3\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Data load error: {e}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # AWS Sagemaker: Downloding a datasets and storing it in S3\n",
    "# try:\n",
    "#     model_data = pd.read_csv(\"./data.csv\", index_col=0)\n",
    "#     print(f\"Success: data loaded from S3\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Data load error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy: 1.19.4\n",
      "Pandas: 1.0.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# import xgboost as xgb\n",
    "\n",
    "print(f\"Numpy: {np.__version__}\")\n",
    "print(f\"Pandas: {pd.__version__}\")\n",
    "# print(f\"XGBoost: {xgb.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 AWS data load error: [Errno 2] No such file or directory: 's3://gscreenproject/y_val.joblib'\n",
      "Google Colab data load error: No module named 'google.colab'\n",
      "Data dir: /storage/\n"
     ]
    }
   ],
   "source": [
    "# Colab notebook & Gradient paperspace\n",
    "try:\n",
    "    dir = \"\"\n",
    "    y_val = joblib.load(\"s3://gscreenproject/\" + f\"y_val.joblib\")\n",
    "    print(f\"Success: data loaded from S3 AWS bucket\")\n",
    "except Exception as e:\n",
    "    print(f\"S3 AWS data load error: {e}\")\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        drive.mount(\"/content/drive\")\n",
    "    except Exception as e:\n",
    "        print(f\"Google Colab data load error: {e}\")\n",
    "        dir = \"/storage/\"\n",
    "    else:\n",
    "        dir = \"/content/drive/MyDrive/Colab/gscreen/\"\n",
    "\n",
    "print(f\"Data dir: {dir}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_fit: (296721, 279), <class 'scipy.sparse.csr.csr_matrix'>\n",
      "X_train: (296727, 279), <class 'scipy.sparse.csr.csr_matrix'>\n",
      "X_val: (5000, 279), <class 'scipy.sparse.csr.csr_matrix'>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load preprocessed data from S3\n",
    "dir=\"\"\n",
    "s3client = boto3.client('s3')\n",
    "\n",
    "s3client.download_file(bucket, 'data_processed/X_fit.joblib.compressed', 'X_fit.joblib.compressed')\n",
    "with open(dir + f\"X_fit.joblib.compressed\", \"rb\") as f:\n",
    "    X_fit = joblib.load(f)\n",
    "\n",
    "s3client.download_file(bucket, 'data_processed/X_train.joblib.compressed', 'X_train.joblib.compressed')\n",
    "with open(dir + f\"X_train.joblib.compressed\", \"rb\") as f:\n",
    "    X_train = joblib.load(f)\n",
    "\n",
    "s3client.download_file(bucket, 'data_processed/X_val.joblib.compressed', 'X_val.joblib.compressed')\n",
    "with open(dir + f\"X_val.joblib.compressed\", 'rb') as f:\n",
    "    X_val = joblib.load(f)\n",
    "\n",
    "print(f\"X_fit: {X_fit.shape}, {type(X_fit)}\\\n",
    "\\nX_train: {X_train.shape}, {type(X_train)}\\\n",
    "\\nX_val: {X_val.shape}, {type(X_val)}\\n\")\n",
    "\n",
    "s3client.download_file(bucket, 'data_processed/y_fit.joblib', 'y_fit.joblib')\n",
    "with open(dir + f\"y_fit.joblib\", \"rb\") as f:\n",
    "    y_fit = joblib.load(dir + f\"y_fit.joblib\")\n",
    "\n",
    "# s3client.download_file(bucket, 'data_processed/y_train.joblib', 'y_train.joblib')\n",
    "# y_train = joblib.load(dir + f\"y_train.joblib\")\n",
    "\n",
    "# s3client.download_file(bucket, 'data_processed/y_val.joblib', 'y_val.joblib')\n",
    "# y_val = joblib.load(dir + f\"y_val.joblib\")\n",
    "\n",
    "# y_fit = pd.Series(y_fit)\n",
    "# print(f\"y_fit: {len(y_fit)}\")\n",
    "# print(f\"y_fit: {len(pd.Series(y_fit))}\\ny_train: {len(y_train)}\\ny_val: {len(y_val)}\")\n",
    "# print(f\"y_fit: {type(y_fit)}\\ny_train: {type(y_train)}\\ny_val: {type(y_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-993ea59b3d76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_fit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \"\"\"\n\u001b[1;32m   1047\u001b[0m         \u001b[0;31m# We are explicitly making element iterators.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1048\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"m\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"M\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1049\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_box_datetimelike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_extension_array_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5270\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5271\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5272\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5273\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5274\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "... last 1 frames repeated, from the frame below ...\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5270\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5271\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5272\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5273\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5274\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded"
     ]
    }
   ],
   "source": [
    "sum(y_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'X_fit.joblib.compressed'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-f329590895c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#%% Load data ------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34mf\"X_fit.joblib.compressed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mX_fit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34mf\"X_train.joblib.compressed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'X_fit.joblib.compressed'"
     ]
    }
   ],
   "source": [
    "#%% Load data ------------------------------------------------------------------\n",
    "with open(dir + f\"X_fit.joblib.compressed\", \"rb\") as f:\n",
    "  X_fit = joblib.load(f)\n",
    "\n",
    "with open(dir + f\"X_train.joblib.compressed\", \"rb\") as f:\n",
    "  X_train = joblib.load(f)\n",
    "\n",
    "with open(dir + f\"X_val.joblib.compressed\", \"rb\") as f:\n",
    "  X_val = joblib.load(f)\n",
    "\n",
    "print(f\"X_fit: {X_fit.shape}, {type(X_fit)}\\\n",
    "\\nX_train: {X_train.shape}, {type(X_train)}\\\n",
    "\\nX_val: {X_val.shape}, {type(X_val)}\\n\")\n",
    "\n",
    "y_fit = joblib.load(dir + f\"y_fit.joblib\")\n",
    "y_train = joblib.load(dir + f\"y_train.joblib\")\n",
    "y_val = joblib.load(dir + f\"y_val.joblib\")\n",
    "\n",
    "print(f\"y_fit: {y_fit.size}, {type(y_fit)}\\\n",
    "\\ny_train: {y_train.size}, {type(y_train)}\\\n",
    "\\ny_val: {y_val.size}, {type(y_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: xgboost\n",
      "randomized_grid_search: False\n",
      "canonical_grid_search: True\n"
     ]
    }
   ],
   "source": [
    "model_name = \"xgboost\"\n",
    "random_state = 42\n",
    "randomized_grid_search = False\n",
    "canonical_grid_search = True\n",
    "\n",
    "print(f\"model_name: {model_name}\")\n",
    "print(f\"randomized_grid_search: {randomized_grid_search}\")\n",
    "print(f\"canonical_grid_search: {canonical_grid_search}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xgb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-3937b750f0e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;34m\"random_state\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m }\n\u001b[0;32m---> 13\u001b[0;31m model = xgb.XGBRegressor(\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;34m**\u001b[0m\u001b[0mmodel_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xgb' is not defined"
     ]
    }
   ],
   "source": [
    "#%% Define model parameters for starting tuning\n",
    "model_params = {\n",
    "    \"tree_method\": \"gpu_hist\",\n",
    "    \"gpu_id\": 0,\n",
    "    \"booster\": \"gbtree\",\n",
    "    \"n_estimators\": 2500,\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    # \"objective\": \"reg:pseudohubererror\",\n",
    "    # \"objective\": \"reg:gamma\",\n",
    "    \"n_jobs\": None,\n",
    "    \"random_state\": random_state,\n",
    "}\n",
    "model = xgb.XGBRegressor(\n",
    "    **model_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters' distributions tune in case RANDOMIZED grid search\n",
    "## Dictionary with parameters names (str) as keys and distributions\n",
    "## or lists of parameters to try.\n",
    "## If a list is given, it is sampled uniformly.\n",
    "param_dist = [\n",
    "    {\n",
    "        \"n_estimators\": [x for x in range(2250, 3001, 250)],\n",
    "        # ^ subsample: default=1. Lower ratios avoid over-fitting\n",
    "        \"subsample\": [x / 10 for x in range(5, 11, 1)],\n",
    "        # ^ \"colsample_bytree: default=1. Lower ratios avoid over-fitting.\n",
    "        \"colsample_bytree\": [x / 10 for x in range(6, 11, 1)],\n",
    "        # ^ max_depth: default=6. Lower ratios avoid over-fitting.\n",
    "        \"max_depth\": [x for x in range(6, 51, 4)],\n",
    "        # ^ min_child_weight: default=1. Larger values avoid over-fitting.\n",
    "        \"min_child_weight\": [1] + [x for x in range(2, 11, 2)],\n",
    "        # ^ Eta (lr): default=0.3. Lower values avoid over-fitting.\n",
    "        \"learning_rate\": [0.001, 0.01, 0.1, 0.2, 0.3],\n",
    "        # ^ Lambda: default=1. Larger values avoid over-fitting.\n",
    "        \"reg_lambda\": [1],  # + [x for x in range(2, 11, 1)],\n",
    "        # ^ Gamma: default=0. Larger values avoid over-fitting.\n",
    "        \"gamma\": [0.0],  # + [x/10 for x in range(5, 60, 5)],\n",
    "    }\n",
    "]\n",
    "# Parameters what we wish to tune in case SIMPLE grid search\n",
    "## Dictionary with parameters names (str) as keys\n",
    "## and lists of parameter settings to try as values\n",
    "param_grid = [\n",
    "    {\n",
    "        \"n_estimators\": [2400, 2500, 2600],\n",
    "        # ^ subsample: default=1. Lower ratios avoid over-fitting\n",
    "#         \"subsample\": [0.6, 0.8],\n",
    "        # ^ \"colsample_bytree: default=1. Lower ratios avoid over-fitting.\n",
    "#         \"colsample_bytree\": [0.6, 0.8],\n",
    "        # ^ max_depth: default=6. Lower ratios avoid over-fitting.\n",
    "        \"max_depth\": [6, 12, 24],\n",
    "        # ^ min_child_weight: default=1. Larger values avoid over-fitting.\n",
    "#         \"min_child_weight\": [1] + [x for x in range(2, 11, 2)],\n",
    "        # ^ Eta (lr): default=0.3. Lower values avoid over-fitting.\n",
    "        \"learning_rate\": [0.01, 0.1, 0.3],\n",
    "        # ^ Lambda: default=1. Larger values avoid over-fitting.\n",
    "#         \"reg_lambda\": [0.5, 1, 2],\n",
    "        # ^ Gamma: default=0. Larger values avoid over-fitting.\n",
    "#         \"gamma\": [0, 1, 2, 5],\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(real_rates, predicted_rates):\n",
    "    \"\"\"Project's accuracy value estimator\"\"\"\n",
    "    return np.average(abs(real_rates / predicted_rates - 1.0)) * 100.0\n",
    "\n",
    "def calc_metrics(model, X, y):\n",
    "    \"\"\"Calculates result metrics\"\"\"\n",
    "\n",
    "    from sklearn.metrics import max_error\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "    y_true = y\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    me = max_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    mare = accuracy(y_true, y_pred)\n",
    "\n",
    "    print(f\"\\tMax Error: {me}\")\n",
    "    print(f\"\\tMean Absolute Error: {mae}\")\n",
    "    print(f\"\\tRoot Mean Squared Error: {rmse}\")\n",
    "    print(f\"\\tMean Absolute Ratio Error: {mare}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Define custom scorer to evaluate basic models\n",
    "scorer = make_scorer(\n",
    "    score_func=accuracy,\n",
    "    greater_is_better=True,  # Whether score_func is a score function (default),\n",
    "    # meaning high is good, or a loss function, meaning low is good.\n",
    ")\n",
    "# Specific fitting parameters. #!Set early stopping to avoid overfitting\n",
    "early_stopping_params = {\n",
    "    \"early_stopping_rounds\": 20,\n",
    "    \"eval_metric\": \"mae\",\n",
    "    # \"eval_metric\": \"mape\",\n",
    "    # \"eval_metric\": \"rmse\",\n",
    "    \"eval_set\": [(X_val, y_val)],\n",
    "}\n",
    "\n",
    "#%% Define Cross Validation parameters for grid search\n",
    "# Define classic cross validation method and params\n",
    "cv = model_selection.RepeatedKFold(\n",
    "    n_splits=4,  #! Dont forget\n",
    "    n_repeats=1,\n",
    "    random_state=random_state,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 30 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "#%% ---------------------- RandomizedSearchCV ----------------------------------\n",
    "rs_bp, rs_be = {}, {}\n",
    "if randomized_grid_search:\n",
    "    tic = time.time()\n",
    "    # Define RANDOMIZED grid search\n",
    "    random_search = model_selection.RandomizedSearchCV(\n",
    "        model,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=30,  #! default=10 Number of parameter settings that are sampled.\n",
    "        scoring=scorer,\n",
    "        n_jobs=None,\n",
    "        cv=cv,\n",
    "        refit=True,\n",
    "        return_train_score=True,\n",
    "        verbose=8,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "    # Make RANDOMIZED grid search\n",
    "    model_random_search = random_search.fit(\n",
    "        X_fit,\n",
    "        y_fit,\n",
    "    )\n",
    "    # Print out best parameters\n",
    "    rs_bp = model_random_search.best_params_\n",
    "    rs_be = model_random_search.best_estimator_\n",
    "    print(f\"\\nRandomized search:\\nBest params are:\\n {rs_bp}\")\n",
    "    print(f\"\\nBest estimator is:\\n {rs_be}\")\n",
    "\n",
    "    min, sec = divmod(time.time() - tic, 60)\n",
    "    print(f\"\\nRandomized grid search taken: {int(min)}min {int(sec)}sec\")\n",
    "\n",
    "    # Print out results\n",
    "    print(f\"{model_name.title()} random search:\")\n",
    "    print(\"TRAIN set:\")\n",
    "    calc_metrics(model_random_search, X=X_train, y=y_train)\n",
    "    print(\"VALIDATION set:\")\n",
    "    calc_metrics(model_random_search, X=X_val, y=y_val)\n",
    "\n",
    "    model = model_random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% --------------------------- GridSearchCV -----------------------------------\n",
    "#! Dont forget to fine set up the Grid Search parameters in algo_*.py module\n",
    "#! and reload that module\n",
    "gs_bp, gs_be = {}, {}\n",
    "if canonical_grid_search:\n",
    "    tic = time.time()\n",
    "    # Define SIMPLE grid search\n",
    "    grid_search = model_selection.GridSearchCV(\n",
    "        model,\n",
    "        param_grid=param_grid,\n",
    "        scoring=scorer,\n",
    "        n_jobs=None,  # Number of jobs to run in parallel. -1 = using all processors\n",
    "        cv=cv,  # Determines the cross-validation splitting strategy.\n",
    "        refit=True,  # Refit model with the best found parameters on the whole dataset.\n",
    "        return_train_score=True,\n",
    "        verbose=8,  # Controls the verbosity: the higher, the more messages.\n",
    "    )\n",
    "    # Make SIMPLE grid search\n",
    "    model_grid_search = grid_search.fit(\n",
    "        X_fit,\n",
    "        y_fit,\n",
    "    )\n",
    "    # Print out best parameters\n",
    "    gs_bp = model_grid_search.best_params_\n",
    "    print(f\"Grid search:\\nBest params are:\\n {gs_bp}\")\n",
    "\n",
    "    min, sec = divmod(time.time() - tic, 60)\n",
    "    print(f\"\\nGrid search taken: {int(min)}min {int(sec)}sec\")\n",
    "\n",
    "    # Print out results\n",
    "    print(f\"{model_name.title()} grid search:\")\n",
    "    print(\"TRAIN set:\")\n",
    "    calc_metrics(model_grid_search, X=X_train, y=y_train)\n",
    "    print(\"VALIDATION set:\")\n",
    "    calc_metrics(model_grid_search, X=X_val, y=y_val)\n",
    "\n",
    "    model = model_grid_search.best_estimator_\n",
    "    print(f\"{model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Fit the model w\\o parameters searching -------------------------------------\n",
    "if not randomized_grid_search and not canonical_grid_search:\n",
    "    # Train the model\n",
    "    tic = time.time()\n",
    "    model.fit(X_fit, y_fit)\n",
    "    # Evaluate time spent\n",
    "    min, sec = divmod(time.time() - tic, 60)\n",
    "    print(f\"Time taken: {int(min)}min {int(sec)}sec\")\n",
    "\n",
    "    # Print out results\n",
    "    print(f\"{model_name.title()} model:\")\n",
    "    print(\"TRAIN set:\")\n",
    "    calc_metrics(model, X=X_train, y=y_train)\n",
    "    print(\"VALIDATION set:\")\n",
    "    calc_metrics(model, X=X_val, y=y_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}