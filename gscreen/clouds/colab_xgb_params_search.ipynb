{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: xgboost\n",
      "randomized_grid_search: True\n",
      "canonical_grid_search: False\n"
     ]
    }
   ],
   "source": [
    "model_name = \"xgboost\"\n",
    "random_state = 42\n",
    "randomized_grid_search = False\n",
    "canonical_grid_search = False\n",
    "bayesian_search = True\n",
    "\n",
    "print(f\"model_name: {model_name}\")\n",
    "print(f\"randomized_grid_search: {randomized_grid_search}\")\n",
    "print(f\"canonical_grid_search: {canonical_grid_search}\")\n",
    "print(f\"bayesian_search: {bayesian_search}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY FOR COLAB. Reinstall default XGBoost\n",
    "from packaging import version\n",
    "import xgboost as xgb\n",
    "\n",
    "if version.parse(xgb.__version__) < version.parse(\"1.3.1\"):\n",
    "  print(f\"XGBoost: {xgb.__version__}. Try to re-install.\")\n",
    "  \n",
    "  try:\n",
    "    !pip uninstall xgboost\n",
    "    !pip install xgboost\n",
    "    print(f\"XGBoost: {xgb.__version__}.\\nYou need to restart kernel.\")\n",
    "  except Exception as e:\n",
    "    print(f\"Re-install XGBoost error: {e}\")\n",
    "\n",
    "else:\n",
    "  print(f\"XGBoost: {xgb.__version__}. Ok.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "except:\n",
    "    dir = \"/storage/\"\n",
    "else:\n",
    "    dir = \"/content/drive/MyDrive/Colab/gscreen/\"\n",
    "\n",
    "print(f\"Data dir: {dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  from skopt import BayesSearchCV\n",
    "  print(f\"The scikit-optimize package exist.\")\n",
    "except Exception as e:\n",
    "  print(f\"Re-install XGBoost error: {e}\")\n",
    "  print(f\"Try to re-install scikit-optimize package\")\n",
    "  ! pip install scikit-optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "print(f\"Numpy: {np.__version__}\")\n",
    "print(f\"Pandas: {pd.__version__}\")\n",
    "print(f\"XGBoost: {xgb.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_fit: (296721, 279)\n",
      "X_train: (296727, 279)\n",
      "X_val: (5000, 279)\n",
      "\n",
      "y_fit: 296721\n",
      "y_train: 296727\n",
      "y_val: 5000\n"
     ]
    }
   ],
   "source": [
    "#%% Load data ------------------------------------------------------------------\n",
    "with open(dir + f\"X_fit.joblib.compressed\", \"rb\") as f:\n",
    "  X_fit = joblib.load(f)\n",
    "\n",
    "with open(dir + f\"X_train.joblib.compressed\", \"rb\") as f:\n",
    "  X_train = joblib.load(f)\n",
    "\n",
    "with open(dir + f\"X_val.joblib.compressed\", \"rb\") as f:\n",
    "  X_val = joblib.load(f)\n",
    "\n",
    "print(f\"X_fit: {X_fit.shape}, {type(X_fit)}\\\n",
    "\\nX_train: {X_train.shape}, {type(X_train)}\\\n",
    "\\nX_val: {X_val.shape}, {type(X_val)}\\n\")\n",
    "\n",
    "y_fit = joblib.load(dir + f\"y_fit.joblib\")\n",
    "y_train = joblib.load(dir + f\"y_train.joblib\")\n",
    "y_val = joblib.load(dir + f\"y_val.joblib\")\n",
    "\n",
    "print(f\"y_fit: {y_fit.size}, {type(y_fit)}\\\n",
    "\\ny_train: {y_train.size}, {type(y_train)}\\\n",
    "\\ny_val: {y_val.size}, {type(y_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Define model parameters for starting tuning\n",
    "model_params = {\n",
    "    # \"tree_method\": \"gpu_hist\",\n",
    "    # \"gpu_id\": 0,\n",
    "    \"booster\": \"gbtree\",\n",
    "    \"n_estimators\": 2500,\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    # \"objective\": \"reg:pseudohubererror\",\n",
    "    # \"objective\": \"reg:gamma\",\n",
    "    \"n_jobs\": None,\n",
    "    \"random_state\": random_state,\n",
    "}\n",
    "model = xgb.XGBRegressor(\n",
    "    **model_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% ---------------------- RandomizedSearchCV ----------------------------------\n",
    "# Parameters' distributions tune in case RANDOMIZED grid search\n",
    "## Dictionary with parameters names (str) as keys and distributions\n",
    "## or lists of parameters to try.\n",
    "## If a list is given, it is sampled uniformly.\n",
    "param_dist = {\n",
    "    \"n_estimators\": [x for x in range(2250, 3001, 250)],\n",
    "    # ^ subsample: default=1. Lower ratios avoid over-fitting\n",
    "    \"subsample\": [x / 10 for x in range(5, 11, 1)],\n",
    "    # ^ \"colsample_bytree: default=1. Lower ratios avoid over-fitting.\n",
    "    \"colsample_bytree\": [x / 10 for x in range(6, 11, 1)],\n",
    "    # ^ max_depth: default=6. Lower ratios avoid over-fitting.\n",
    "    \"max_depth\": [x for x in range(6, 51, 4)],\n",
    "    # ^ min_child_weight: default=1. Larger values avoid over-fitting.\n",
    "    \"min_child_weight\": [1] + [x for x in range(2, 11, 2)],\n",
    "    # ^ Eta (lr): default=0.3. Lower values avoid over-fitting.\n",
    "    \"learning_rate\": [0.001, 0.01, 0.1, 0.2, 0.3],\n",
    "    # ^ Lambda: default=1. Larger values avoid over-fitting.\n",
    "    \"reg_lambda\": [1],  # + [x for x in range(2, 11, 1)],\n",
    "    # ^ Gamma: default=0. Larger values avoid over-fitting.\n",
    "    \"gamma\": [0.0],  # + [x/10 for x in range(5, 60, 5)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% --------------------------- GridSearchCV -----------------------------------\n",
    "# Parameters what we wish to tune in case SIMPLE grid search\n",
    "## Dictionary with parameters names (str) as keys\n",
    "## and lists of parameter settings to try as values\n",
    "param_grid = {\n",
    "    \"n_estimators\": [2400, 2500, 2600],\n",
    "    # ^ subsample: default=1. Lower ratios avoid over-fitting\n",
    "    \"subsample\": [0.6, 0.8],\n",
    "    # ^ \"colsample_bytree: default=1. Lower ratios avoid over-fitting.\n",
    "    \"colsample_bytree\": [0.6, 0.8],\n",
    "    # ^ max_depth: default=6. Lower ratios avoid over-fitting.\n",
    "    \"max_depth\": [6, 12, 24],\n",
    "    # ^ min_child_weight: default=1. Larger values avoid over-fitting.\n",
    "    \"min_child_weight\": [1] + [x for x in range(2, 11, 2)],\n",
    "    # ^ Eta (lr): default=0.3. Lower values avoid over-fitting.\n",
    "    \"learning_rate\": [0.01, 0.1, 0.3],\n",
    "    # ^ Lambda: default=1. Larger values avoid over-fitting.\n",
    "    \"reg_lambda\": [0.5, 1, 2],\n",
    "    # ^ Gamma: default=0. Larger values avoid over-fitting.\n",
    "    \"gamma\": [0, 1, 2, 5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% ----------------------- Bayesian Optimization ------------------------------\n",
    "# Their core idea of Bayesian Optimization is simple:\n",
    "# when a region of the space turns out to be good, it should be explored more.\n",
    "# Real: Continuous hyperparameter space.\n",
    "# Integer: Discrete hyperparameter space.\n",
    "# Categorical: Categorical hyperparameter space.\n",
    "bayes_space = {\n",
    "    # \"n_estimators\": Integer(2000, 3000),\n",
    "    \"subsample\": Real(0.6, 1.0),\n",
    "    \"colsample_bytree\": Real(0.7, 1.0),\n",
    "    \"max_depth\": Integer(3, 20),\n",
    "    \"min_child_weight\": Integer(1, 20),\n",
    "    \"learning_rate\": Real(0.01, 0.4),\n",
    "    \"reg_lambda\": Real(0.5, 5),\n",
    "    \"gamma\": Real(0.0, 5),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(real_rates, predicted_rates):\n",
    "    \"\"\"Project's accuracy value estimator\"\"\"\n",
    "    return np.average(abs(real_rates / predicted_rates - 1.0)) * 100.0\n",
    "\n",
    "def calc_metrics(model, X, y):\n",
    "    \"\"\"Calculates result metrics\"\"\"\n",
    "\n",
    "    from sklearn.metrics import max_error\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "    y_true = y\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    me = max_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    mare = accuracy(y_true, y_pred)\n",
    "\n",
    "    print(f\"\\tMax Error: {me}\")\n",
    "    print(f\"\\tMean Absolute Error: {mae}\")\n",
    "    print(f\"\\tRoot Mean Squared Error: {rmse}\")\n",
    "    print(f\"\\tMean Absolute Ratio Error: {mare}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Define custom scorer to evaluate basic models\n",
    "scorer = make_scorer(\n",
    "    score_func=accuracy,\n",
    "    greater_is_better=True,  # Whether score_func is a score function (default),\n",
    "    # meaning high is good, or a loss function, meaning low is good.\n",
    ")\n",
    "# Specific fitting parameters. #!Set early stopping to avoid overfitting\n",
    "early_stopping_params = {\n",
    "    \"early_stopping_rounds\": 20,\n",
    "    \"eval_metric\": \"mae\",\n",
    "    # \"eval_metric\": \"mape\",\n",
    "    # \"eval_metric\": \"rmse\",\n",
    "    \"eval_set\": [(X_val, y_val)],\n",
    "}\n",
    "\n",
    "#%% Define Cross Validation parameters for grid search\n",
    "# Define classic cross validation method and params\n",
    "cv = model_selection.RepeatedKFold(\n",
    "    n_splits=3,  #! Dont forget\n",
    "    n_repeats=1,\n",
    "    random_state=random_state,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 30 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "#%% ---------------------- RandomizedSearchCV ----------------------------------\n",
    "rs_bp, rs_be = {}, {}\n",
    "if randomized_grid_search:\n",
    "    tic = time.time()\n",
    "    # Define RANDOMIZED grid search\n",
    "    random_search = model_selection.RandomizedSearchCV(\n",
    "        model,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=30,  #! default=10 Number of parameter settings that are sampled.\n",
    "        scoring=scorer,\n",
    "        n_jobs=None,\n",
    "        cv=cv,\n",
    "        refit=True,\n",
    "        return_train_score=True,\n",
    "        verbose=8,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "    # Make RANDOMIZED grid search\n",
    "    model_random_search = random_search.fit(\n",
    "        X_fit,\n",
    "        y_fit,\n",
    "    )\n",
    "    # Print out best parameters\n",
    "    rs_bp = model_random_search.best_params_\n",
    "    rs_be = model_random_search.best_estimator_\n",
    "    print(f\"\\nRandomized search:\\nBest params are:\\n {rs_bp}\")\n",
    "    print(f\"\\nBest estimator is:\\n {rs_be}\")\n",
    "\n",
    "    min, sec = divmod(time.time() - tic, 60)\n",
    "    print(f\"\\nRandomized grid search taken: {int(min)}min {int(sec)}sec\")\n",
    "\n",
    "    # Print out results\n",
    "    print(f\"{model_name.title()} random search:\")\n",
    "    print(\"TRAIN set:\")\n",
    "    calc_metrics(model_random_search, X=X_train, y=y_train)\n",
    "    print(\"VALIDATION set:\")\n",
    "    calc_metrics(model_random_search, X=X_val, y=y_val)\n",
    "\n",
    "    model = model_random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% --------------------------- GridSearchCV -----------------------------------\n",
    "#! Dont forget to fine set up the Grid Search parameters in algo_*.py module\n",
    "#! and reload that module\n",
    "gs_bp, gs_be = {}, {}\n",
    "if canonical_grid_search:\n",
    "    tic = time.time()\n",
    "    # Define SIMPLE grid search\n",
    "    grid_search = model_selection.GridSearchCV(\n",
    "        model,\n",
    "        param_grid=param_grid,\n",
    "        scoring=scorer,\n",
    "        n_jobs=None,  # Number of jobs to run in parallel. -1 = using all processors\n",
    "        cv=cv,  # Determines the cross-validation splitting strategy.\n",
    "        refit=True,  # Refit model with the best found parameters on the whole dataset.\n",
    "        return_train_score=True,\n",
    "        verbose=8,  # Controls the verbosity: the higher, the more messages.\n",
    "    )\n",
    "    # Make SIMPLE grid search\n",
    "    model_grid_search = grid_search.fit(\n",
    "        X_fit,\n",
    "        y_fit,\n",
    "    )\n",
    "    # Print out best parameters\n",
    "    gs_bp = model_grid_search.best_params_\n",
    "    print(f\"Grid search:\\nBest params are:\\n {gs_bp}\")\n",
    "\n",
    "    min, sec = divmod(time.time() - tic, 60)\n",
    "    print(f\"\\nGrid search taken: {int(min)}min {int(sec)}sec\")\n",
    "\n",
    "    # Print out results\n",
    "    print(f\"{model_name.title()} grid search:\")\n",
    "    print(\"TRAIN set:\")\n",
    "    calc_metrics(model_grid_search, X=X_train, y=y_train)\n",
    "    print(\"VALIDATION set:\")\n",
    "    calc_metrics(model_grid_search, X=X_val, y=y_val)\n",
    "\n",
    "    model = model_grid_search.best_estimator_\n",
    "    print(f\"{model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% ----------------------- Bayesian Optimization ------------------------------\n",
    "bs_bp, bs_be = {}, {}\n",
    "if bayesian_search:\n",
    "    tic = time.time()\n",
    "    # Define bayesian search\n",
    "    bayes_search = BayesSearchCV(\n",
    "        model,\n",
    "        search_spaces=bayes_space,\n",
    "        n_iter=50,  #! default=50 Number of parameter settings that are sampled.\n",
    "        scoring=scorer,\n",
    "        n_jobs=None,  # Number of jobs to run in parallel. -1 = using all processors\n",
    "        cv=3,  # default 3-fold cross validation.\n",
    "        refit=True,\n",
    "        verbose=3,\n",
    "        return_train_score=True,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "    # Make search\n",
    "    model_bayes_search = bayes_search.fit(\n",
    "        X_fit,\n",
    "        y_fit,\n",
    "    )\n",
    "    # Print out best parameters\n",
    "    bs_bp = model_bayes_search.best_params_\n",
    "    print(f\"Bayesian search:\\nBest params are:\\n {bs_bp}\")\n",
    "\n",
    "    min, sec = divmod(time.time() - tic, 60)\n",
    "    print(f\"\\nBayesian search taken: {int(min)}min {int(sec)}sec\")\n",
    "\n",
    "    # Print out results\n",
    "    print(f\"{model_name.title()} Bayesian search:\")\n",
    "    print(\"TRAIN set:\")\n",
    "    calc_metrics(model_bayes_search, X=X_train, y=y_train)\n",
    "    print(\"VALIDATION set:\")\n",
    "    calc_metrics(model_bayes_search, X=X_val, y=y_val)\n",
    "\n",
    "    model = model_bayes_search.best_estimator_\n",
    "    print(f\"{model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Fit the model w\\o parameters searching -------------------------------------\n",
    "if not randomized_grid_search and not canonical_grid_search and not bayesian_search:\n",
    "    # Train the model\n",
    "    tic = time.time()\n",
    "    model.fit(X_fit, y_fit)\n",
    "    # Evaluate time spent\n",
    "    min, sec = divmod(time.time() - tic, 60)\n",
    "    print(f\"Time taken: {int(min)}min {int(sec)}sec\")\n",
    "    print(f\"{model}\\n\")\n",
    "\n",
    "    # Print out results\n",
    "    print(f\"{model_name.title()} model:\")\n",
    "    print(\"TRAIN set:\")\n",
    "    calc_metrics(model, X=X_train, y=y_train)\n",
    "    print(\"VALIDATION set:\")\n",
    "    calc_metrics(model, X=X_val, y=y_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}